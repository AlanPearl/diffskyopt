{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dataclasses\n",
    "from mpi4py import MPI\n",
    "import numpy as np\n",
    "import jax.numpy as jnp\n",
    "import jax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Diffskyopt Tutorial: Fitting to COSMOS + Extending to New Datasets\n",
    "\n",
    "This tutorial demonstrates how to use  `diffopt` (kdescent + multigrad) to fit `diffsky` to COSMOS and showing how to extend to new datasets like BGS. Code blocks are not executable to reduce clutter and prioritize learning the important components. We can manually add the missing pieces as an exercise and verify everything executes as expected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generating and Distributing Diffsky Halos with MPI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffskyopt.diffsky_model import generate_weighted_grid_lc_data, lc_data_slice\n",
    "\n",
    "SIZE, RANK = MPI.COMM_WORLD.size, MPI.COMM_WORLD.rank\n",
    "\n",
    "if RANK == 0:\n",
    "    # Generate the full lightcone only on rank 0\n",
    "    # (TODO: generate dithered grids on each rank)\n",
    "    full_lc_data, full_halo_upweights = generate_weighted_grid_lc_data(\n",
    "        z_min=0.4, z_max=2.0, num_z_grid=100,\n",
    "        lgmp_min=10.5, lgmp_max=15.0, num_m_grid=100,\n",
    "        sky_area_degsq=1.21, ran_key=jax.random.key(0)\n",
    "    )\n",
    "    indices = np.array_split(np.arange(full_lc_data.z_obs.size), SIZE)\n",
    "    lc_data_slices = [lc_data_slice(full_lc_data, idx) for idx in indices]\n",
    "    halo_upweights_slices = [full_halo_upweights[idx] for idx in indices]\n",
    "else:\n",
    "    lc_data_slices = None\n",
    "    halo_upweights_slices = None\n",
    "\n",
    "# Each rank receives only its slice\n",
    "lc_data = MPI.COMM_WORLD.scatter(lc_data_slices, root=0)\n",
    "halo_upweights = MPI.COMM_WORLD.scatter(halo_upweights_slices, root=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Defining the COSMOS Loss Function\n",
    "\n",
    "The core fitting logic is encapsulated in a class, e.g. [`CosmosFit`](../diffskyopt/lossfuncs/cosmos_fit.py). This class loads the data, prepares targets, and defines the loss function using kdescent and multigrad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffopt import kdescent, multigrad\n",
    "from cosmos20_colors import load_cosmos20\n",
    "from diffsky.param_utils import diffsky_param_wrapper as dpw\n",
    "\n",
    "class CosmosFit:\n",
    "    def __init__(self, ...):\n",
    "        # Load and mask COSMOS data\n",
    "        cat = load_cosmos20()\n",
    "        # Mask out NaNs and select redshift range\n",
    "        # Prepare data_targets and data_weights\n",
    "        self.data_targets, self.data_weights = self._prepare_data_targets_and_weights(cat)\n",
    "\n",
    "        # Distribute model halos as above\n",
    "        # self.lc_data, self.halo_upweights = ... (see previous section)\n",
    "\n",
    "        # Pretrain kdescent kernels for KDE and Fourier loss terms\n",
    "        ktrain = kdescent.KPretrainer.from_training_data(\n",
    "            self.data_targets, self.data_weights,\n",
    "            num_eval_kernels=40, num_eval_fourier_positions=20,\n",
    "            comm=MPI.COMM_WORLD\n",
    "        )\n",
    "        self.kcalc = kdescent.KCalc(ktrain)\n",
    "\n",
    "    def targets_and_weights_from_params(self, params, randkey):\n",
    "        # Each rank computes photometry for its slice\n",
    "        targets, weights = compute_targets_and_weights(\n",
    "            params, self.lc_data,\n",
    "            ran_key=jax.random.split(randkey, SIZE)[RANK],\n",
    "            weights=self.halo_upweights\n",
    "        )\n",
    "        return targets, weights\n",
    "\n",
    "    def sumstats_from_params(self, params, randkey):\n",
    "        keys = jax.random.split(randkey, 3)\n",
    "        model_targets, model_weights = self.targets_and_weights_from_params(\n",
    "            params, keys[0])\n",
    "        model_k, data_k, err_k = self.kcalc.compare_kde_counts(\n",
    "            keys[1], model_targets, model_weights, return_err=True)\n",
    "        model_f, data_f, err_f = self.kcalc.compare_fourier_counts(\n",
    "            keys[2], model_targets, model_weights, return_err=True)\n",
    "        sumstats = jnp.concatenate([model_k, model_f])\n",
    "        sumstats_aux = jnp.concatenate([data_k, err_k, data_f, err_f])\n",
    "        return sumstats, sumstats_aux\n",
    "\n",
    "    def loss_from_sumstats(self, sumstats, sumstats_aux):\n",
    "        # Compute reduced chi^2 or similar metric\n",
    "        normalized_residuals = (sumstats - sumstats_aux[:len(sumstats)]) / sumstats_aux[len(sumstats):]\n",
    "        return jnp.mean(normalized_residuals**2)\n",
    "\n",
    "    def get_multi_grad_calc(self):\n",
    "        return self.MultiGradModel(aux_data=dict(fit_instance=self))\n",
    "\n",
    "    @dataclasses.dataclass\n",
    "    class MultiGradModel(multigrad.OnePointModel):\n",
    "        aux_data: dict\n",
    "        sumstats_func_has_aux: bool = True\n",
    "\n",
    "        def calc_partial_sumstats_from_params(self, params, randkey):\n",
    "            fit_instance = self.aux_data[\"fit_instance\"]\n",
    "            return fit_instance.sumstats_from_params(params, randkey)\n",
    "\n",
    "        def calc_loss_from_sumstats(self, sumstats, sumstats_aux, randkey=None):\n",
    "            fit_instance = self.aux_data[\"fit_instance\"]\n",
    "            return fit_instance.loss_from_sumstats(sumstats, sumstats_aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Fitting the Model with Adam Optimization\n",
    "\n",
    "To support parallelization, use the multigrad optimizer to minimize the loss function. This is typically run in an MPI job (see [`fit.pbs`](/home/apearl/jobs/diffskyopt/cosmos_fit/fit.pbs)) but in principle you can run it on a single rank in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_fit = CosmosFit(...)\n",
    "calc = cosmos_fit.get_multi_grad_calc()\n",
    "key = jax.random.key(1)\n",
    "params, losses = calc.run_adam(\n",
    "    guess=cosmos_fit.default_u_param_arr,\n",
    "    nsteps=1000, learning_rate=0.1,\n",
    "    randkey=key, progress=True\n",
    ")\n",
    "# Save results for validation\n",
    "np.savez(\"cosmos_fit_results.npz\", params=np.asarray(params), losses=np.asarray(losses))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Validating the Fit\n",
    "\n",
    "After fitting, generate diagnostic plots using the validation scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffskyopt.scripts import cosmos_validation\n",
    "\n",
    "cosmos_validation.n_of_z_plot(cosmos_fit, model_params=params[-1])\n",
    "cosmos_validation.n_of_ithresh_plot(cosmos_fit, model_params=params[-1])\n",
    "cosmos_validation.smhm_drift_plot(model_params=params[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Extending to More Datasets: BGS Example\n",
    "\n",
    "Let's try simultaneously fitting to COSMOS and BGS data. You can create a new class `BGSFit` with the same structure as `CosmosFit`, but loading BGS data from a dummy `load_bgs` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BGSFit:\n",
    "    def __init__(self, ...):\n",
    "        # Load and mask BGS data\n",
    "        cat = load_bgs()\n",
    "        self.data_targets, self.data_weights = self._prepare_data_targets_and_weights(cat)\n",
    "        # Distribute model halos as above\n",
    "        # Pretrain kdescent kernels\n",
    "        ktrain = kdescent.KPretrainer.from_training_data(\n",
    "            self.data_targets, self.data_weights,\n",
    "            num_eval_kernels=40, num_eval_fourier_positions=20,\n",
    "            comm=MPI.COMM_WORLD\n",
    "        )\n",
    "        self.kcalc = kdescent.KCalc(ktrain)\n",
    "        # ...\n",
    "\n",
    "    def _prepare_data_targets_and_weights(self, cat):\n",
    "        # Return data_targets and data_weights in the same format as CosmosFit\n",
    "        ...\n",
    "\n",
    "    # All other methods can be inherited from CosmosFit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Joint Fitting: Combining Losses\n",
    "\n",
    "To fit both datasets simultaneously, simply sum their loss terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosmos_fit = CosmosFit(...)\n",
    "bgs_fit = BGSFit(...)\n",
    "\n",
    "\n",
    "def combined_loss(params, randkey):\n",
    "    cosmos_sumstats, cosmos_aux = cosmos_fit.sumstats_from_params(\n",
    "        params, randkey)\n",
    "    bgs_sumstats, bgs_aux = bgs_fit.sumstats_from_params(params, randkey)\n",
    "    loss_cosmos = cosmos_fit.loss_from_sumstats(cosmos_sumstats, cosmos_aux)\n",
    "    loss_bgs = bgs_fit.loss_from_sumstats(bgs_sumstats, bgs_aux)\n",
    "    return loss_cosmos + loss_bgs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In parallel, we will need to make use of MultiGrad\n",
    "cosmos_multigrad = cosmos_fit.get_multi_grad_calc()\n",
    "bgs_multigrad = bgs_fit.get_multi_grad_calc()\n",
    "\n",
    "# experimental\n",
    "multigroup = multigrad.OnePointGroup(\n",
    "    (cosmos_multigrad, bgs_multigrad))\n",
    "\n",
    "# computes the sum of the loss of each multigrad model in the group\n",
    "multigroup.calc_loss_and_grad_from_params(cosmos_fit.default_u_param_arr)\n",
    "\n",
    "# run gradient descent\n",
    "multigroup.run_adam(\n",
    "    guess=cosmos_fit.default_u_param_arr,\n",
    "    nsteps=1000, learning_rate=0.1,\n",
    "    randkey=key, progress=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "main312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
